from fastapi import APIRouter, HTTPException, Path
from typing import List, Dict, Any
import logging
import uuid
from datetime import datetime

from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

from src.BMO.api.schemas import ChatRequest, ChatResponse, HistoryResponse, MessageHistory, HealthResponse
from src.BMO.core.orchestrator import get_compiled_graph, get_graph_config
from src.BMO.config.settings import settings

router = APIRouter()
logger = logging.getLogger(__name__)

@router.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    Process a user message and return the AI response.
    
    This endpoint:
    1. Initializes/retreives a session.
    2. Invokes the LangGraph workflow.
    3. Persists the conversation via the graph's checkpointer.
    """
    session_id = request.session_id or str(uuid.uuid4())
    logger.info(f"API Chat request - Session: {session_id}")
    
    try:
        graph = get_compiled_graph()
        config = get_graph_config(session_id)
        
        # Prepare input for the graph
        inputs = {"messages": [HumanMessage(content=request.message)]}
        
        # Invoke the graph synchronously (for now, can be improved to stream later)
        # Note: orchestrator's graph uses MemorySaver/SqliteSaver
        result = graph.invoke(inputs, config)
        
        # Extract the last AI message
        messages = result.get("messages", [])
        if not messages:
            raise HTTPException(status_code=500, detail="No response generated by BMO")
            
        last_message = messages[-1]
        if not isinstance(last_message, AIMessage):
             # Try to find the last AI message if the graph flow ended differently
             ai_messages = [m for m in messages if isinstance(m, AIMessage)]
             last_message = ai_messages[-1] if ai_messages else messages[-1]

        return ChatResponse(
            response=last_message.content,
            session_id=session_id
        )
        
    except Exception as e:
        logger.error(f"Error in /chat endpoint: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/history/{session_id}", response_model=HistoryResponse)
async def get_history(session_id: str = Path(..., description="The session ID to retrieve history for")):
    """Retrieve message history for a specific session."""
    try:
        graph = get_compiled_graph()
        config = get_graph_config(session_id)
        
        # Get state from checkpointer
        state = graph.get_state(config)
        messages = state.values.get("messages", [])
        
        history = []
        for msg in messages:
            role = "unknown"
            if isinstance(msg, HumanMessage): role = "human"
            elif isinstance(msg, AIMessage): role = "ai"
            elif isinstance(msg, SystemMessage): role = "system"
            
            history.append(MessageHistory(
                role=role,
                content=msg.content
            ))
            
        return HistoryResponse(
            session_id=session_id,
            messages=history
        )
    except Exception as e:
        logger.error(f"Error in /history endpoint: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/health", response_model=HealthResponse)
async def health():
    """Check API and system health."""
    return HealthResponse(
        status="ok",
        version="0.1.0"
    )
